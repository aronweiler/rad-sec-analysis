# RAD Security Analysis - Default Configuration
# This configuration supports dynamic LLM selection per stage, along with some other configurations.

# Global Application Settings
debug: false
log_level: "INFO"

incident_parser: "json_v1"

# Token Management
global_token_budget: null  # No global limit by default
enable_token_tracking: true

# Caching Configuration
cache_backend: "memory"
cache_config:
  memory:
    max_size: 1000
    ttl_seconds: 3600
  # these are not used, but included for completeness
  redis:
    host: "localhost"
    port: 6379
    db: 0
  file:
    cache_dir: "./cache"
    max_files: 1000

# Stage Configurations
# Each stage can use a different LLM provider and model (or none at all).
stages:
  incident_pre_processing:
    stage: "incident_pre_processing"
    enabled: true
    settings:
      strict_version_matching: true # strict version matching for CVEs

  # incident_research:
  #   stage: "incident_research"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-3-5-haiku-latest"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 4096
  #     timeout: 30
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.9
  #   max_final_retries: 3
  #   max_iterations: 3
  #   available_tools: ["search_cves_by_keyword", "get_cve_details", "submit_research"]
  #   available_mcp_servers: ["vulnerability_intelligence"]
  incident_research:
    stage: "incident_research"
    enabled: true
    llm_config:
      provider: "openai"
      model_name: "gpt-4o"
      temperature: 0 # Deterministic as possible
      max_tokens: 4096
      timeout: 30
      max_retries: 10
      extra_params:
        top_p: 0.9
        frequency_penalty: 0.0
    max_final_retries: 3
    max_iterations: 3
    available_tools: ["search_cves_by_keyword", "get_cve_details", "submit_research"]
    available_mcp_servers: ["vulnerability_intelligence"]

  incident_analysis:
    stage: "incident_analysis"
    enabled: true
    llm_config:
      provider: "openai"
      model_name: "gpt-4o"
      temperature: 0 # Deterministic as possible
      max_tokens: 8192
      timeout: 30
      max_retries: 10
      extra_params:
        top_p: 0.9
    max_final_retries: 3
    max_iterations: 3
    available_tools: ["submit_analysis"]
    available_mcp_servers: ["vulnerability_intelligence"]
  # incident_analysis:
  #   stage: "incident_analysis"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-sonnet-4-0"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 8192
  #     timeout: 30
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.9
  #   max_final_retries: 3
  #   max_iterations: 3
  #   available_tools: ["submit_analysis"]
  #   available_mcp_servers: ["vulnerability_intelligence"]

  report_generation:
    stage: "report_generation"
    enabled: true
    settings:
      output_directory: "./reports"

  # Other possible stages- if we were to extend the pipeline
  # prioritized_risk_and_impact_assessment:
  #   stage: "prioritized_risk_and_impact_assessment"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-sonnet-4-0"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 3000
  #     timeout: 45
  #     max_retries: 10
  #     extra_params:
  #       top_k: 40
  #   token_budget: 3000
  #   enable_caching: true
  #   cache_ttl: 7200
  #   max_iterations: 1
  #   available_tools: ["risk_calculator", "impact_analyzer"]

  # final_incident_analysis:
  #   stage: "final_incident_analysis"
  #   enabled: true
  #   llm_config:
  #     provider: "openai"
  #     model_name: "gpt-4.1"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 2500
  #     timeout: 60
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.95
  #   token_budget: 2500
  #   enable_caching: true
  #   cache_ttl: 1800
  #   max_iterations: 1
  #   available_tools: ["report_generator", "summary_tool"]
    

# MCP Server Configurations
mcp_servers:
  vulnerability_intelligence:
    name: "vulnerability_intelligence"
    host: "localhost"  
    port: 8000
    transport_type: "streamable_http"
    command: []  # Not needed for external server
    enabled: true
    timeout: 30
    env_vars: {}
