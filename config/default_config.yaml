# RAD Security Analysis - Default Configuration
# This configuration supports dynamic LLM selection per stage, along with some other configurations.

# Global Application Settings
debug: false
log_level: "INFO"

incident_parser: "json_v1"

# Token Management
global_token_budget: null  # No global limit by default
enable_token_tracking: true

# Caching Configuration
cache_backend: "memory"
cache_config:
  memory:
    max_size: 1000
    ttl_seconds: 3600
  # these are not used, but included for completeness
  redis:
    host: "localhost"
    port: 6379
    db: 0
  file:
    cache_dir: "./cache"
    max_files: 1000

# Stage Configurations
# Each stage can use a different LLM provider and model (or none at all).
stages:
  incident_pre_processing:
    stage: "incident_pre_processing"
    enabled: true
    settings:
      strict_version_matching: true # strict version matching for CVEs
      max_cves_per_software: 2000 # Max CVEs capable of being returned by NVD database in one call
      max_age_days: 365 # Maximum age of CVEs to consider
      prioritize_recent_days: 30 # Prioritize CVEs from the last 30 days
      min_relevance_score: 0.7 # Minimum relevance score for CVEs
      lookback_years: 2 # Look back at most 2 years for CVEs
      post_incident_days: 90 # Post-incident day CVEs to consider

  cpe_extraction:
    stage: "cpe_extraction"
    enabled: true
    llm_config:
      provider: "ollama"
      model_name: "llama3.2:latest"  
      temperature: 0  # Deterministic for consistent CPE generation
      max_tokens: 4096
      timeout: 60  # Longer timeout for batch processing
      max_retries: 5
      extra_params:
        top_p: 0.9
    max_final_retries: 3
    max_iterations: 1  # Auto-terminate after one iteration, since we're processing all of the incidents at once (stage will batch them)
    available_tools: ["generate_cpes_for_batch"]
    settings:
      # Batch processing configuration
      asset_batch_size: 20  # Number of incidents per batch

      # Validation thresholds for preventing hallucinations
      hostname_similarity_threshold: 0.8
      software_name_similarity_threshold: 0.7
      software_version_similarity_threshold: 0.8
      vendor_product_similarity_threshold: 0.6

      # Validation strictness settings
      strict_ip_matching: true  # Require exact IP address matches
      strict_hostname_matching: false  # Allow similarity-based hostname matching

  incident_research:
    stage: "incident_research"
    enabled: true
    llm_config:
      provider: "openai"
      model_name: "gpt-4o"
      temperature: 0 # Deterministic as possible
      max_tokens: 4096
      timeout: 30
      max_retries: 10
      extra_params:
        top_p: 0.9
        frequency_penalty: 0.0
    max_final_retries: 3
    max_iterations: 3
    available_tools: ["search_cves_by_keyword", "get_cve_details", "submit_research"]
    available_mcp_servers: ["vulnerability_intelligence"]

  # incident_research:
  #   stage: "incident_research"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-3-5-haiku-latest"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 4096
  #     timeout: 30
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.9
  #   max_final_retries: 3
  #   max_iterations: 3
  #   available_tools: ["search_cves_by_keyword", "get_cve_details", "submit_research"]
  #   available_mcp_servers: ["vulnerability_intelligence"]

  incident_analysis:
    stage: "incident_analysis"
    enabled: true
    llm_config:
      provider: "openai"
      model_name: "gpt-4o"
      temperature: 0 # Deterministic as possible
      max_tokens: 8192
      timeout: 30
      max_retries: 10
      extra_params:
        top_p: 0.9
    max_final_retries: 3
    max_iterations: 3
    available_tools: ["submit_analysis"]
    available_mcp_servers: ["vulnerability_intelligence"]
  # incident_analysis:
  #   stage: "incident_analysis"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-sonnet-4-0"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 8192
  #     timeout: 30
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.9
  #   max_final_retries: 3
  #   max_iterations: 3
  #   available_tools: ["submit_analysis"]
  #   available_mcp_servers: ["vulnerability_intelligence"]

  report_generation:
    stage: "report_generation"
    enabled: true
    settings:
      output_directory: "./reports"

  # Other possible stages- if we were to extend the pipeline
  # prioritized_risk_and_impact_assessment:
  #   stage: "prioritized_risk_and_impact_assessment"
  #   enabled: true
  #   llm_config:
  #     provider: "anthropic"
  #     model_name: "claude-sonnet-4-0"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 3000
  #     timeout: 45
  #     max_retries: 10
  #     extra_params:
  #       top_k: 40
  #   token_budget: 3000
  #   enable_caching: true
  #   cache_ttl: 7200
  #   max_iterations: 1
  #   available_tools: ["risk_calculator", "impact_analyzer"]

  # final_incident_analysis:
  #   stage: "final_incident_analysis"
  #   enabled: true
  #   llm_config:
  #     provider: "openai"
  #     model_name: "gpt-4.1"
  #     temperature: 0 # Deterministic as possible
  #     max_tokens: 2500
  #     timeout: 60
  #     max_retries: 10
  #     extra_params:
  #       top_p: 0.95
  #   token_budget: 2500
  #   enable_caching: true
  #   cache_ttl: 1800
  #   max_iterations: 1
  #   available_tools: ["report_generator", "summary_tool"]
    

# MCP Server Configurations
mcp_servers:
  vulnerability_intelligence:
    name: "vulnerability_intelligence"
    host: "localhost"  
    port: 8000
    transport_type: "streamable_http"
    command: []  # Not needed for external server
    enabled: true
    timeout: 30
    env_vars: {}
